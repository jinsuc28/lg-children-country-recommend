{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Merge IDEA\n",
    "\n",
    "1. historyì— ëŒ€í•´ drop duplicate í›„ drop column      \n",
    "2. watch ì— ëŒ€í•´ì„œë„ drop duplicate í›„ drop column     \n",
    "3. ì´í›„ inner join       \n",
    "4. ë‹¤ë¥¸ dataset merge   \n",
    "\n",
    "ë¬¸ì œ : 3ë²ˆë¶€í„° ë¨ì´ í„°ì ¸ë‚˜ê°    \n",
    "ëŒ€ì•ˆ 1. ë‚˜ëˆ ì„œ ì¡°ì¸ (ë…¸ê°€ë‹¤...)   \n",
    "ëŒ€ì•ˆ 2. ë” ë‚˜ì€ ë…¸íŠ¸ë¶ í™˜ê²½ ì°¾ê¸°   \n",
    "ëŒ€ì•ˆ 3. ë˜‘ë˜‘í•˜ê²Œ ë¨¸ë¦¬ ì“°ê¸° **<í•´ê²° ë°©ì•ˆ ì°¾ì•„ëƒ„>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-12T02:47:19.700466Z",
     "iopub.status.busy": "2022-11-12T02:47:19.699956Z",
     "iopub.status.idle": "2022-11-12T02:47:19.707583Z",
     "shell.execute_reply": "2022-11-12T02:47:19.705618Z",
     "shell.execute_reply.started": "2022-11-12T02:47:19.700424Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‹œì²­ ì‹œì‘ ë°ì´í„°ì—ì„œ row ì¤‘ë³µ ì²´í¬ - 1ë§Œ 6ì²œê°œ ê°€ëŸ‰ì˜ ì¤‘ë³µ í™•ì¸. Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:12.227696Z",
     "iopub.status.busy": "2022-11-12T02:20:12.227326Z",
     "iopub.status.idle": "2022-11-12T02:20:12.237643Z",
     "shell.execute_reply": "2022-11-12T02:20:12.236696Z",
     "shell.execute_reply.started": "2022-11-12T02:20:12.227653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nì˜ì‹¬ì˜ ì—¬ì§€ ì—†ì´ ì¤‘ë³µ ì¡´ì¬ \\nì•„ë˜ ì½”ë“œ ì‹¤í–‰ ì‹œ groupbyë¡œ ìƒë‹¹íˆ ë§ì´ ë¬¶ì´ê³ , ìµœì¢… column ê°œìˆ˜ê°€ 899021ë¡œ ì¤„ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ì˜ì‹¬ì˜ ì—¬ì§€ ì—†ì´ ì¤‘ë³µ ì¡´ì¬ \n",
    "ì•„ë˜ ì½”ë“œ ì‹¤í–‰ ì‹œ groupbyë¡œ ìƒë‹¹íˆ ë§ì´ ë¬¶ì´ê³ , ìµœì¢… column ê°œìˆ˜ê°€ 899021ë¡œ ì¤„ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ\n",
    "''' \n",
    "# history.groupby(['profile_id','log_time','ss_id']).count().sort_values('album_id',ascending=False)[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:12.239561Z",
     "iopub.status.busy": "2022-11-12T02:20:12.238850Z",
     "iopub.status.idle": "2022-11-12T02:20:13.236332Z",
     "shell.execute_reply": "2022-11-12T02:20:13.235138Z",
     "shell.execute_reply.started": "2022-11-12T02:20:12.239518Z"
    }
   },
   "outputs": [],
   "source": [
    "history = pd.read_csv('data/history_data.csv')\n",
    "history.drop_duplicates(subset=['profile_id','log_time','album_id'],inplace=True) \n",
    "# row = 1005651 rows Ã— 8 columns\n",
    "# after drop duplicates 899021 rows Ã— 8 columns\n",
    "# 106630 ê°œ row ì¤‘ë³µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:13.241053Z",
     "iopub.status.busy": "2022-11-12T02:20:13.240348Z",
     "iopub.status.idle": "2022-11-12T02:20:13.245508Z",
     "shell.execute_reply": "2022-11-12T02:20:13.244383Z",
     "shell.execute_reply.started": "2022-11-12T02:20:13.241016Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'continuous_play','short_trailer','log_time' -- drop\n",
    "# payment ê²°ì¸¡ 0ìœ¼ë¡œ ëŒ€ì²´\n",
    "# history.drop(labels=['log_time','act_target_dtl','continuous_play','short_trailer'],axis=1).fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watchì— ëŒ€í•´ì„œë„ ë™ì¼í•œ ì‘ì—… ìˆ˜í–‰ (duplicate drop, feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:13.247043Z",
     "iopub.status.busy": "2022-11-12T02:20:13.246706Z",
     "iopub.status.idle": "2022-11-12T02:20:14.023413Z",
     "shell.execute_reply": "2022-11-12T02:20:14.022395Z",
     "shell.execute_reply.started": "2022-11-12T02:20:13.247012Z"
    }
   },
   "outputs": [],
   "source": [
    "watch = pd.read_csv('data/watch_e_data.csv')\n",
    "watch.drop_duplicates(subset=['profile_id','log_time','album_id'],inplace=True) \n",
    "# watch.drop(labels=['log_time','act_target_dtl'],axis=1,inplace=True)  # column ë“œë. ë”°ë¡œ ì²˜ë¦¬í•  ê²°ì¸¡ì¹˜ ì—†ìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:14.024900Z",
     "iopub.status.busy": "2022-11-12T02:20:14.024596Z",
     "iopub.status.idle": "2022-11-12T02:20:14.030136Z",
     "shell.execute_reply": "2022-11-12T02:20:14.029137Z",
     "shell.execute_reply.started": "2022-11-12T02:20:14.024870Z"
    }
   },
   "outputs": [],
   "source": [
    "# full ë³‘í•© ì½”ë“œëŠ” í­íƒ„ ê·¸ ìì²´\n",
    "# ëŒë ¸ë‹¤ í•˜ë©´ í„°ì ¸ìš© \n",
    "# datamerge = pd.merge(history,watch,how='inner',on='profile_id')\n",
    "# datamerge.to_csv('../input/lgground')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í„°ì§„ë‹¤...! ğŸ’£ğŸ’¥\n",
    "*ê·¼ë° ì—¬ê¸°ì„œ ë¬¸ë“ ë“œëŠ” ìƒê°*\n",
    "\n",
    "1. ë°˜ë“œì‹œ historyì™€ watchë¥¼ inner joinìœ¼ë¡œ ë¬¶ì–´ì•¼ í• ê¹Œ? ê·¸ëŸ¬ë‹ˆê¹Œ... serveí•  ë¹„ì¤‘ë³µ ìœ ì € ë¦¬ìŠ¤íŠ¸ë§Œ historyì—ì„œ ê°€ì ¸ì˜¤ê³ , ì œëŒ€ë¡œ ëœ neg/pos samplingì´ ê°€ëŠ¥í•œ watch ë°ì´í„°ì— ë¶™ì—¬ì„œ ê·¸ê²ƒ ì¤‘ì‹¬ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ë„ ìˆì§€ ì•Šë‚˜?     \n",
    "ì• ì´ˆì— historyì—ì„œ ìœ ì˜ë¯¸í•œ columnì€ **ì‹œì²­ ì‹œì‘ ë°ì´í„°ì—ë§Œ ìˆëŠ” profile_id & ê·¸ë‚˜ë§ˆ payment ë¿ì´ë‹¤**    \n",
    "ã„´ *ê·¸ë‚˜ë§ˆ payment* : paymentëŠ” ì˜ìƒ ì‹œì²­ì˜ ì¥ë²½ ê°™ì€ ìš”ì†Œë¡œ ì‘ë™í•˜ê¸° ë•Œë¬¸ì— (ë¬´ë£Œ ì˜ìƒì´ ìœ ë£Œ ì˜ìƒë³´ë‹¤ ë§ì´ ì¬ìƒëœë‹¤) sampling í˜¹ì€ ì¶”ì²œ ìš”ì†Œì— ì¤‘ìš”í•˜ê²Œ ê³ ë ¤í•  ë§Œ í•˜ë‹¤.    \n",
    ".       \n",
    "           \n",
    "ì´ë ‡ê²Œ ì ‘ê·¼í•˜ë©´ ì–´ë–¤ ë°©ì‹ì´ ê°€ëŠ¥í•´ì§€ëƒë©´ < log_time columnë„ ë‚ ë¦´ ìˆ˜ ìˆê²Œ ëœë‹¤ :: ê·¸ë˜ë„ train-vaildë¥¼ ìœ„í•´ì„œ ë‚ ë¦¬ì§„ ë§ì >    \n",
    "1. ë‘ datasetì—ì„œ í•„ìš” ì—†ëŠ” columnì„ dropí•˜ê³        \n",
    "2. historyì—ë§Œ ìˆëŠ” profile_id ê°€ ìˆëŠ” rowë¥¼ ì „ë¶€ ì¶”ì¶œí•´ watchì— appendí•œë‹¤.\n",
    "3. ì¶”í›„ ë°ì´í„°ì…‹ ë¶„í• ì„ ê³ ë ¤í•´, log_time ê¸°ì¤€ìœ¼ë¡œ dataframe ì •ë ¬ sort.    \n",
    "3. ìœ„ ë°ì´í„°ì…‹ì— meta + profile ë°ì´í„°ì…‹ merge ìˆ˜í–‰í•œë‹¤.    \n",
    "     \n",
    "> ìµœì¢…ì ìœ¼ë¡œëŠ” watchì— ìˆëŠ” ì¤‘ìš”í•œ ê°’ì„ ê¸°ë³¸ìœ¼ë¡œ ê°€ì§€ê³ , serveëŒ€ìƒì¸ ìœ ì €ëª©ë¡ê¹Œì§€ ì±™ê¸´ datasetì´ ë¨     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:14.031553Z",
     "iopub.status.busy": "2022-11-12T02:20:14.031251Z",
     "iopub.status.idle": "2022-11-12T02:20:14.523098Z",
     "shell.execute_reply": "2022-11-12T02:20:14.522021Z",
     "shell.execute_reply.started": "2022-11-12T02:20:14.031525Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/2dv1p69j3pv2c66bj9jt_hbc0000gn/T/ipykernel_61340/2297796919.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = watch.append(id_only_in_history_rows,sort=False).sort_values('log_time').reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "# historyì—ë§Œ ìˆëŠ” profile_id  Length: 8311 / watchì—ë§Œ ìˆëŠ” profile_id  Length: 7658\n",
    "m_his = history['profile_id'].drop_duplicates() # í„°ì§ ë°©ì§€ë¡œ profile_id columnë§Œ ë‚¨ê¹€ \n",
    "m_wat = watch['profile_id'].drop_duplicates()\n",
    "id_only_in_history = pd.merge(m_his,m_wat,how='outer',indicator=True\n",
    "                ).query('_merge == \"left_only\"').drop(columns=['_merge'])\n",
    "id_only_in_history_list = id_only_in_history['profile_id'].to_list()\n",
    "id_only_in_history_rows = history[history['profile_id'].isin(id_only_in_history_list)]\n",
    "id_only_in_history_rows = id_only_in_history_rows.drop_duplicates(subset=['profile_id','ss_id','log_time','album_id']) # 15241 rows\n",
    "id_only_in_history_rows.drop(columns=['continuous_play','short_trailer'])\n",
    "dataset = watch.append(id_only_in_history_rows,sort=False).sort_values('log_time').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:14.525113Z",
     "iopub.status.busy": "2022-11-12T02:20:14.524710Z",
     "iopub.status.idle": "2022-11-12T02:20:14.541700Z",
     "shell.execute_reply": "2022-11-12T02:20:14.540544Z",
     "shell.execute_reply.started": "2022-11-12T02:20:14.525070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15241\n",
      "8311\n"
     ]
    }
   ],
   "source": [
    "# dataset verification\n",
    "# watch : 800632 +15241 = row 815873 ë‚˜ì™€ì•¼ ì •ìƒë³‘í•©\n",
    "print(dataset['total_time'].isnull().sum()) #ì œëŒ€ë¡œ í•©ì³ì¡Œë‹¤ë©´ ê²°ì¸¡ì´ 15241ê°œ ìˆì„ ê²ƒ\n",
    "print(dataset['profile_id'].nunique()) #ì œëŒ€ë¡œ í•©ì³ì¡Œë‹¤ë©´ 8311ëª…ì¼ ê²ƒ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:28:44.936875Z",
     "iopub.status.busy": "2022-11-12T02:28:44.936095Z",
     "iopub.status.idle": "2022-11-12T02:28:45.057008Z",
     "shell.execute_reply": "2022-11-12T02:28:45.055814Z",
     "shell.execute_reply.started": "2022-11-12T02:28:44.936839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 815873 entries, 0 to 815872\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   profile_id       815873 non-null  int64  \n",
      " 1   ss_id            815873 non-null  int64  \n",
      " 2   log_time         815873 non-null  int64  \n",
      " 3   act_target_dtl   815873 non-null  object \n",
      " 4   album_id         815873 non-null  int64  \n",
      " 5   watch_time       800632 non-null  float64\n",
      " 6   total_time       800632 non-null  float64\n",
      " 7   continuous_play  815873 non-null  object \n",
      " 8   payment          564 non-null     float64\n",
      " 9   short_trailer    15241 non-null   object \n",
      "dtypes: float64(3), int64(4), object(3)\n",
      "memory usage: 62.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Data merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:47:27.172413Z",
     "iopub.status.busy": "2022-11-12T02:47:27.171998Z",
     "iopub.status.idle": "2022-11-12T02:47:28.120125Z",
     "shell.execute_reply": "2022-11-12T02:47:28.118766Z",
     "shell.execute_reply.started": "2022-11-12T02:47:27.172379Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/2dv1p69j3pv2c66bj9jt_hbc0000gn/T/ipykernel_61340/3753341068.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  meta = pd.read_csv('data/meta_data.csv') # 42602 rows\n"
     ]
    }
   ],
   "source": [
    "# # ë©”íƒ€ë°ì´í„° ë³‘í•© \n",
    "meta = pd.read_csv('data/meta_data.csv') # 42602 rows\n",
    "meta_p = pd.read_csv('data/meta_data_plus.csv') # 767948 rows\n",
    "# meta_merge = pd.merge(meta,meta_p,how='inner',on='album_id') #### meta ì •ë³´ ìµœì¢… ì§‘í•©ì²´ : 832356 rows\n",
    "# data_w_meta = pd.merge(dataset,meta,how='inner',on='album_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° \n",
    "class cfg: \n",
    "    gpu_idx = 0\n",
    "    top_k = 25\n",
    "    seed = 42\n",
    "    neg_ratio = 100\n",
    "    test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/2dv1p69j3pv2c66bj9jt_hbc0000gn/T/ipykernel_61340/2801991363.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['rating'] = 1\n"
     ]
    }
   ],
   "source": [
    "data = dataset[['profile_id','album_id']]\n",
    "data['rating'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ë°ì´í„° í¬ê¸°: (652698, 3)\n",
      "ê²€ì¦ ë°ì´í„° í¬ê¸°: (163175, 3)\n"
     ]
    }
   ],
   "source": [
    "cfg.n_users = data.profile_id.max()+1\n",
    "cfg.n_items = data.album_id.max()+1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "from tqdm.notebook import tqdm\n",
    "# í•™ìŠµ ë° ê²€ì¦ ë°ì´í„° ë¶„ë¦¬\n",
    "train, valid = train_test_split(\n",
    "    data, test_size=cfg.test_size, random_state=cfg.seed,\n",
    ")\n",
    "print('í•™ìŠµ ë°ì´í„° í¬ê¸°:', train.shape)\n",
    "print('ê²€ì¦ ë°ì´í„° í¬ê¸°:', valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009532928466796875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 652698,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdebe3c36884aff8552bfcfe46098af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/652698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train í˜•íƒœ: \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix í˜•íƒœë¡œ ë³€í™˜ \n",
    "train = train.to_numpy()\n",
    "matrix = sparse.lil_matrix((cfg.n_users, cfg.n_items))\n",
    "for (p, i, r) in tqdm(train):\n",
    "    matrix[p, i] = r\n",
    "    \n",
    "train = sparse.csr_matrix(matrix)\n",
    "train = train.toarray()\n",
    "print(\"train í˜•íƒœ: \\n\", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_UIdataset(train, neg_ratio):\n",
    "    \"\"\" ìœ ì €ë³„ í•™ìŠµì— í•„ìš”í•œ ë”•ì…”ë„ˆë¦¬ ë°ì´í„° ìƒì„± \n",
    "    Args:\n",
    "        train : ìœ ì €-ì•„ì´í…œì˜ ìƒí˜¸ì‘ìš©ì„ ë‹´ì€ í–‰ë ¬ \n",
    "            ex) \n",
    "                array([[0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        ...,\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.]])\n",
    "        neg_ratio : negative sampling í™œìš©í•  ë¹„ìœ¨ \n",
    "            ex) 3 (positive label 1ê°œë‹¹ negative label 3ê°œ)\n",
    "    Returns: \n",
    "        UIdataset : ìœ ì €ë³„ í•™ìŠµì— í•„ìš”í•œ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ \n",
    "            ex) {'ì‚¬ìš©ì ID': [[positive ìƒ˜í”Œ, negative ìƒ˜í”Œ], ... , [1, 1, 1, ..., 0, 0]]}\n",
    "                >>> UIdataset[3]\n",
    "                    [array([   16,    17,    18, ...,  9586, 18991,  9442]),\n",
    "                    array([5, 5, 5, ..., 5, 5, 5]),\n",
    "                    array([4, 4, 4, ..., 5, 1, 1]),\n",
    "                    array([1., 1., 1., ..., 0., 0., 0.])]\n",
    "    \"\"\"\n",
    "    UIdataset = {'profile_id': [], 'album_id':[],'rating':[]}\n",
    "    for user_id, items_by_user in enumerate(train):\n",
    "#         UIdataset[user_id] = []\n",
    "        # positive ìƒ˜í”Œ ê³„ì‚° \n",
    "        pos_item_ids = np.where(items_by_user > 0.5)[0]\n",
    "        num_pos_samples = len(pos_item_ids)\n",
    "\n",
    "        # negative ìƒ˜í”Œ ê³„ì‚° (random negative sampling) \n",
    "        num_neg_samples = neg_ratio * num_pos_samples\n",
    "\n",
    "        neg_items = np.where(items_by_user < 0.5)[0]\n",
    "        neg_item_ids = np.random.choice(neg_items, min(num_neg_samples, len(neg_items)), replace=False)\n",
    "        UIdataset['album_id'] += list(neg_item_ids)+list(pos_item_ids)\n",
    "        UIdataset['profile_id'] += [user_id for _ in range(len(neg_item_ids)+len(pos_item_ids))]\n",
    "\n",
    "#         for column in columns:\n",
    "#             # feature ì¶”ì¶œ \n",
    "#             features = []\n",
    "#             for item_id in np.concatenate([pos_item_ids, neg_item_ids]): \n",
    "#                 features.append(user_features[column][user_id])\n",
    "#             UIdataset[column].append(np.array(features))\n",
    "\n",
    "#             features = []\n",
    "#             for item_id in np.concatenate([pos_item_ids, neg_item_ids]): \n",
    "#                 features.append(item_features[column][item_id])\n",
    "#             UIdataset[column].append(np.array(features))\n",
    "\n",
    "            # label ì €ì¥  \n",
    "        pos_labels = np.ones(len(pos_item_ids))\n",
    "        neg_labels = np.zeros(len(neg_item_ids))\n",
    "        UIdataset['rating'] += list(neg_labels)+list(pos_labels)\n",
    "\n",
    "    return UIdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_neg_samplings = make_UIdataset(train,100)\n",
    "pos_neg_samplings_df = pd.DataFrame(pos_neg_samplings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta[['album_id','title','genre_mid','run_time']].drop_duplicates(subset=['album_id'], ignore_index=True)\n",
    "profile = pd.read_csv('data/profile_data.csv')\n",
    "profile_df = profile[['profile_id','sex','age']]\n",
    "\n",
    "dataset_meta_merge = pd.merge(pos_neg_samplings_df, meta_df, how='left', on='album_id')\n",
    "train_df = pd.merge(dataset_meta_merge,profile_df, how='left', on='profile_id')\n",
    "qids_train = train_df.groupby('profile_id')['profile_id'].count().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_dataset_meta_merge = pd.merge(valid, meta_df, how='left', on='album_id')\n",
    "valid_df = pd.merge(valid_dataset_meta_merge,profile_df, how='left', on='profile_id')\n",
    "qids_validation = valid_df.groupby('profile_id')['profile_id'].count().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df, columns=['album_id','title','genre_mid','sex'],sparse=True)\n",
    "valid_df = pd.get_dummies(valid_df, columns=['album_id','title','genre_mid','sex'],sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightgbm rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "\n",
    "\n",
    "X_train = train_df.drop(columns=['rating'])\n",
    "y_train = train_df['rating']\n",
    "\n",
    "X_validation = valid_df.drop(columns=['rating'])\n",
    "y_validation = valid_df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lightgbm.LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    group=qids_train,\n",
    "    eval_set=[(X_validation, y_validation)],\n",
    "    eval_group=[qids_validation],\n",
    "    eval_at=10,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
